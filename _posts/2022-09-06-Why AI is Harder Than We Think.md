---
layout: post
title: Why AI is Harder Than We Think?
summary: "why ai is hard bro"
cover: "/assets/img/why_ai_is_harder_than_we_think/ai.webp"
tags: ["paper review", "news"]
---

> First of all, this blog post is inspired by the paper “Why AI is Harder Than We Think?” whose author is [Melanie Mitchell](https://www.santafe.edu/people/profile/melanie-mitchell){:target="_blank"} who is a Professor at the Santa Fe University and has numerous researches on artificial intelligence and cognitive science. Mitchell has pointed out four fallacies while interpretting the AI and I really encourage you to read the [original paper](https://arxiv.org/abs/2104.12871){:target="_blank"}.

Today, we all know that Artificial Intelligence is a popular topic in both academic researches and the society. This popularity is affecting expectations of AI and future of it and with the improvements in the field, it is expected that a rapid progress towards "General AI" by first the researchers then the society. Theoretically, General AI, in other terms [Artificial General Intelligence (AGI)](https://en.wikipedia.org/wiki/Artificial_general_intelligence){:target="_blank"}, is the use of artificial intelligence in any domain to solve any challenge that calls for intelligence by behaving like a person.

However, Mitchell states that *"Even with today’s seemingly fast pace of AI breakthroughs, the development of long-promised technologies such as self-driving cars, housekeeping robots, and conversational companions has turned out to be much harder than many people expected."* and these situations occur because of people's over-confidence about AI, terminological fallacies while interpreting it and our perception about the intelligence.

Related with those fallacies and misinterpretations, AI expert Drew McDermott talks about a **cyclical pattern** in the field of AI. As we can see in the history of AI, it means that the technology and new initiatives hyped the field of AI and the following investments by governments and startups supported that. But the expectations are stucked in two cycling periods, "AI Spring" and "AI Winter".

![Spring and Winter period of AI](/assets/img/why_ai_is_harder_than_we_think/ai_winter.webp)

## AI Spring and AI Winter

The AI Spring can be described as the hoping period of AI, new approaches have come up and ideas of where the AI can go and which problems it can solve are debated in a positive attitude. Conversely, the AI Winter follows the spring when all the pretentious expectations could not happen and AI lose its confidence.

Lots of innovations have been made by researchers and technology companies throughout time. These innovations started with efforts of [Frank Rosenblatt](https://news.cornell.edu/stories/2019/09/professors-perceptron-paved-way-ai-60-years-too-soon){:target="_blank"} – at the Cornell Aeronautical Laboratory – by combining Donald Hebb’s model of brain cell interaction with Arthur Samuel’s machine learning efforts and created the perceptron model. With the perceptron theorem, optimistic pictures of the [Symbolic AI](http://wiki.pathmind.com/symbolic-reasoning){:target="_blank"}, which is simply an AI system acting like the human brain with defined rules against symbols and abstract concepts, produced by the AI practitioners. But these perceptron models are very limited and the first *AI Winter* appeared after that optimism, the fundings have been disappeared. In early 80s, AI cathed a hype again with "expert systems" such as Japan's "Fifth Generation" project and US's "Strategic Computing Initiative". Expert systems relied on humans to create rules, therefore that kind of a system was unable to generalized towards various domains. As expected, all the fundings decreased again.

About the time after that winter, Mitchell gives an anecdote as;
>"Indeed, the late 1980’s marked the beginning of a new AI winter, and the field’s reputation suffered. When I received my PhD in 1990, I was advised not to use the term “artificial intelligence” on my job applications."

After all those winters, around 2000s, statistics based methods emerged under the roof called [Machine Learning](https://www.ibm.com/cloud/learn/machine-learning){:target="_blank"}, as all we know. These methods are performing specific tasks with "predictive models" rather than building a human-like intelligence. With the developments in the technology and capabilities of the computational power, around 2010, [Deep Learning](https://www.ibm.com/cloud/blog/ai-vs-machine-learning-vs-deep-learning-vs-neural-networks){:target="_blank"} turned back into the business. Actually the Deep Neural Networks showed up around 70s and the first backpropagation demonstration provided by [Yann LeCun](http://yann.lecun.com/){:target="_blank"} at Bell Labs. With today's technology, it is shown that DL systems can solve unsolved AI challanges and that fact exposed the term "AI" in everywhere and a new era of optimism has arrived towards "General AI" or "Human Level AI".

However, today's AI technology, mainly DL, is mostly rely on datasets therefore the AI models can come up with unexpected errors or behaviors. Moreover the systems sometimes can learn some statistical relationship in the data, in other words sometimes learning process may be failed since the systems are not learning the concepts we are trying to teach them they learn some shortcuts, wrong reasons. DL approaches are not *understand* the concepts the way human sense of understand. Mitchell states that, i believe a important point,;

>"It’s still a matter of debate in the AI community whether such understanding can be achieved by adding network layers and more training data, or whether something more fundamental is missing."

There are new DL approaches generating optimism about AI such as Self-Supervised Learning (SSL) or [Meta-Learning](https://machinelearningmastery.com/meta-learning-in-machine-learning/){:target="_blank"}. LeCun writes about SSL and states that "Today, We’re sharing details on why self-supervised learning may be helpful in unlocking [the dark matter of intelligence](https://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/){:target="_blank"} — and the next frontier of AI".

Even though these new approaches, technologies and opportunities provides a progress and excites us, the way to the human-level AI is still longer than represented to the society. There are some lack of understanding and misinterpretation about AI and human intelligence and Mitchell summarized them into four fallacies.

>"While these fallacies are well-known in the AI community, many assumptions made by experts still fall victim to these fallacies, and give us a false sense of confidence about the near-term prospects of "truly" intelligent machines."


![Fallacy One](/assets/img/why_ai_is_harder_than_we_think/narrow_intelligence.webp)

## Fallacy 1: Narrow intelligence is on a continuum with general intelligence

The first fallacy can be named as believing every development is a step through the general intelligence. Of course, every improvement or achievement lets AI to handle another task or existing one more accurately but it still cannot be predicted encountering with an unfortunate obstacle through the way of glorious general AI. That fallacy makes us expect more after every development.

## Fallacy 2: Easy things are easy and hard things are hard

The second fallacy is related with the Moravec’s Paradox which states that easy things that a human can do is actually harder for AI and same goes for hard tasks. This is because the unconscious intelligence and complexity of human thought. Therefore, simple thinking processes are harder to be accomplished by AI since we cannot reason about these tasks.

## Fallacy 3: The lure of wishful mnemonics

The third fallacy is misusing or misinterpreting of terminology that is used about AI. More basically it is about using same words for human actions and AI capabilities. About a common term, when we use learning for an AI most of the people assume that as a human-like learning process, however it is far different from that and based on some mathematical optimizations. These complexities lead by media or some researches and enables societies and even some researches to misinterpret the results of the AI.

## Fallacy 4: Intelligence is all in the brain

The fourth and last fallacy is associating the phenomenon of intelligence with human brain not whole-body interactions. While interpreting human intelligence and AI together only human brain and its acts are considered even the interaction of human body with the nature is known. Forgetting all the emotions and interactions with environment can misled AI to understand the world and can be devastating.

To conclude, I believe that while interpreting results of AI, researches and developments should be done in a scientific sense while considering psychology of human and philosophy of AI.
